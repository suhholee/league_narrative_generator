{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ab1d55",
   "metadata": {},
   "source": [
    "# League of Legends Narrative Generator\n",
    "### Henry Hu, Suhho Lee, Victor Wei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227ad134",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a36fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping champions: 100%|██████████| 170/170 [19:43<00:00,  6.96s/champion]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import re\n",
    "import traceback\n",
    "\n",
    "class LoLChampionScraper:\n",
    "    def __init__(self):\n",
    "        self.champions_url = \"https://universe.leagueoflegends.com/en_US/champions/\"\n",
    "        self.base_url = \"https://universe.leagueoflegends.com\"\n",
    "        self.champions_data = []\n",
    "\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.88 Safari/537.36\") # Example user agent\n",
    "        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=self.chrome_options)\n",
    "        print(\"WebDriver Initialized\")\n",
    "\n",
    "    def extract_champions_list(self):\n",
    "        \"\"\"Extract list of champions using Selenium\"\"\"\n",
    "        self.driver.get(self.champions_url)\n",
    "        time.sleep(5)\n",
    "        selectors = [\n",
    "            \"li.item_30l8 a\",\n",
    "            \".champsListUl_2Lmb li a\",\n",
    "            \"a[href*='/champion/']\"\n",
    "        ]\n",
    "        champions = []\n",
    "        timeout = 10\n",
    "        for selector in selectors:\n",
    "            try:\n",
    "                WebDriverWait(self.driver, timeout).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
    "                )\n",
    "                champion_elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                if champion_elements:\n",
    "                    for element in champion_elements:\n",
    "                        try:\n",
    "                            url = element.get_attribute(\"href\")\n",
    "                            if not url or not url.startswith(self.base_url):\n",
    "                                continue\n",
    "                            name_element = element.find_element(By.CSS_SELECTOR, \"h1\") if element.find_elements(By.CSS_SELECTOR, \"h1\") else None\n",
    "                            region_element = element.find_element(By.CSS_SELECTOR, \"h2\") if element.find_elements(By.CSS_SELECTOR, \"h2\") else None\n",
    "                            name = name_element.text.strip() if name_element and name_element.text else \"\"\n",
    "                            region = region_element.text.strip() if region_element and region_element.text else \"\"\n",
    "                            if name and url and not any(c['name'] == name.upper() for c in champions):\n",
    "                                champions.append({'name': name.upper(), 'region': region, 'url': url})\n",
    "                        except Exception as e: print(f\"  Warn: Error processing a champion list element: {e}\")\n",
    "                    if champions:\n",
    "                        print(f\"  Successfully extracted champion list using selector: {selector}\")\n",
    "                        break\n",
    "            except TimeoutException: print(f\"  Selector {selector} timed out.\")\n",
    "            except Exception as e: print(f\"  Selector {selector} failed with error: {e}\")\n",
    "        print(f\"Found {len(champions)} unique champions\")\n",
    "        return champions\n",
    "\n",
    "    def extract_champion_details(self, champion_data):\n",
    "        \"\"\"Extract detailed information for a specific champion's main page\"\"\"\n",
    "        print(f\"Extracting details for {champion_data['name']}...\")\n",
    "        if not champion_data.get('url'):\n",
    "            print(f\"  Error: Missing URL for {champion_data['name']}\")\n",
    "            return champion_data\n",
    "        try:\n",
    "            self.driver.get(champion_data['url'])\n",
    "            WebDriverWait(self.driver, 15).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"body\")))\n",
    "            time.sleep(1 + random.random())\n",
    "        except Exception as e:\n",
    "            print(f\"  Error navigating to champion page {champion_data['url']}: {e}\")\n",
    "            return champion_data\n",
    "\n",
    "        # Extract Role, Race, Quote, Short Bio\n",
    "        try:\n",
    "            role_element = WebDriverWait(self.driver, 5).until(EC.presence_of_element_located((By.CSS_SELECTOR, \".typeDescription_ixWu h6, .playerType_3laO h6\")))\n",
    "            champion_data['role'] = role_element.text.strip()\n",
    "        except: champion_data['role'] = \"\"\n",
    "        try:\n",
    "            race_elements = self.driver.find_elements(By.CSS_SELECTOR, \".ChampionRace_a_Fp h6, .race_3k58 h6\")\n",
    "            champion_data['race'] = race_elements[0].text.strip() if race_elements else \"\"\n",
    "        except: champion_data['race'] = \"\"\n",
    "        try:\n",
    "            quote_elements = self.driver.find_elements(By.CSS_SELECTOR, \".quote_2507 p, .championQuotes_3FLE p\")\n",
    "            champion_data['quote'] = quote_elements[0].text.strip() if quote_elements else \"\"\n",
    "        except: champion_data['quote'] = \"\"\n",
    "        \n",
    "        # IMPROVED: Extract short bio with multiple selectors and approaches\n",
    "        try:\n",
    "            # First try the original approach\n",
    "            bio_elements = self.driver.find_elements(By.CSS_SELECTOR, \".biographyText_3-to p, .biography_3YIe p\")\n",
    "            if bio_elements and bio_elements[0].text.strip():\n",
    "                champion_data['short_bio'] = bio_elements[0].text.strip()\n",
    "            else:\n",
    "                # Try getting the text from the div container directly\n",
    "                bio_containers = self.driver.find_elements(By.CSS_SELECTOR, \".biographyText_3-to, .biography_3YIe\")\n",
    "                if bio_containers:\n",
    "                    container_text = bio_containers[0].text.strip()\n",
    "                    if container_text:\n",
    "                        # Split by newlines and take the first paragraph if multiple exist\n",
    "                        paragraphs = [p.strip() for p in container_text.split('\\n') if p.strip()]\n",
    "                        if paragraphs:\n",
    "                            champion_data['short_bio'] = paragraphs[0]\n",
    "                        else:\n",
    "                            champion_data['short_bio'] = container_text\n",
    "                    else:\n",
    "                        champion_data['short_bio'] = \"\"\n",
    "                else:\n",
    "                    champion_data['short_bio'] = \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"  Warn: Error extracting short bio: {e}\")\n",
    "            champion_data['short_bio'] = \"\"\n",
    "\n",
    "        # Extract Related Champions\n",
    "        related_champions = []\n",
    "        try:\n",
    "            h5_locator = (By.CSS_SELECTOR, \"ul.champions_jmhN li.champion_1xlO h5\")\n",
    "            WebDriverWait(self.driver, 3).until(EC.presence_of_element_located(h5_locator))\n",
    "            related_elements = self.driver.find_elements(*h5_locator)\n",
    "            if related_elements:\n",
    "                for i, elem in enumerate(related_elements):\n",
    "                    try:\n",
    "                        champion_name = self.driver.execute_script(\"return arguments[0].textContent;\", elem).strip()\n",
    "                        if champion_name and champion_name not in related_champions:\n",
    "                            related_champions.append(champion_name)\n",
    "                    except Exception as inner_e: \n",
    "                        print(f\"    Warn: Error processing related champion element {i+1}: {type(inner_e).__name__} - {inner_e}\")\n",
    "        except Exception as e: \n",
    "            print(f\"  Warn: An unexpected error occurred while finding/processing related champions: {type(e).__name__} - {e}\")\n",
    "        champion_data['related_champions'] = related_champions\n",
    "        print(f\"  Assigned related champions list: {champion_data['related_champions']}\")\n",
    "\n",
    "        # Find Biography URL\n",
    "        try:\n",
    "            bio_link_elements = self.driver.find_elements(By.XPATH, \"//a[.//button[.//span[contains(text(), 'Read Biography') or contains(text(), 'Read Bio')]]]|//a[contains(@href,'/story/champion/')]\")\n",
    "            found_bio_url = \"\"\n",
    "            if bio_link_elements:\n",
    "                for link_el in bio_link_elements:\n",
    "                    href = link_el.get_attribute('href')\n",
    "                    if href and '/story/champion/' in href: found_bio_url = href; break\n",
    "                if not found_bio_url: found_bio_url = bio_link_elements[0].get_attribute('href')\n",
    "            if found_bio_url: champion_data['bio_url'] = found_bio_url\n",
    "            else:\n",
    "                clean_name = re.sub(r'[^a-z0-9]', '', champion_data['name'].lower()); bio_url = f\"{self.base_url}/en_US/story/champion/{clean_name}/\"; champion_data['bio_url'] = bio_url; print(f\"  Warn: Could not find bio button/link, constructed fallback URL: {bio_url}\")\n",
    "        except Exception as e: \n",
    "            print(f\"  Warn: Could not find or construct biography URL: {e}\"); champion_data['bio_url'] = \"\"\n",
    "        champion_data['story_url'] = \"\"\n",
    "        return champion_data\n",
    "\n",
    "    def extract_page_content(self, container_selector, paragraph_selector):\n",
    "        \"\"\"Helper function to extract joined paragraph text from a container.\"\"\"\n",
    "        full_text = \"\"\n",
    "        paragraphs_count = 0\n",
    "        try:\n",
    "            print(f\"  Attempting to find container '{container_selector}' directly in DOM...\")\n",
    "            container_elements = self.driver.find_elements(By.CSS_SELECTOR, container_selector)\n",
    "\n",
    "            if not container_elements:\n",
    "                print(f\"  Error: Container '{container_selector}' not found in DOM after interaction attempt.\")\n",
    "                return full_text, paragraphs_count\n",
    "\n",
    "            container_element = container_elements[0]\n",
    "            print(f\"  Container '{container_selector}' found in DOM.\")\n",
    "\n",
    "            paragraphs = container_element.find_elements(By.CSS_SELECTOR, paragraph_selector)\n",
    "            paragraphs_count = len(paragraphs)\n",
    "            if paragraphs:\n",
    "                extracted_texts = []\n",
    "                for i, p in enumerate(paragraphs):\n",
    "                    try:\n",
    "                        para_text = self.driver.execute_script(\n",
    "                            \"return arguments[0].textContent;\", p\n",
    "                        ).strip()\n",
    "                        if para_text:\n",
    "                            extracted_texts.append(para_text)\n",
    "                    except Exception as inner_e:\n",
    "                        print(f\"    Warn: Error processing paragraph {i+1}: {type(inner_e).__name__} - {inner_e}\")\n",
    "\n",
    "                full_text = \"\\n\\n\".join(extracted_texts)\n",
    "                if not full_text and paragraphs_count > 0:\n",
    "                    print(f\"  Warn: Found {paragraphs_count} paragraphs in '{container_selector}', but all textContent was empty after processing.\")\n",
    "            else:\n",
    "                print(f\"  Warn: Container '{container_selector}' found, but no paragraphs matched selector '{paragraph_selector}'.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error: Exception finding/processing content within '{container_selector}': {type(e).__name__} - {e}\")\n",
    "\n",
    "        return full_text, paragraphs_count\n",
    "\n",
    "    def extract_bio_and_story(self, champion_data):\n",
    "        \"\"\"Extract full biography from bio_url and find the story_url.\"\"\"\n",
    "        champion_data['full_biography'] = \"\"\n",
    "        if not champion_data.get('bio_url'):\n",
    "            print(f\"  Info: No biography URL available for {champion_data['name']}\")\n",
    "            return champion_data\n",
    "\n",
    "        print(f\"Navigating to biography page for {champion_data['name']}...\")\n",
    "        try:\n",
    "            self.driver.get(champion_data['bio_url'])\n",
    "            WebDriverWait(self.driver, 15).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"body\")))\n",
    "            time.sleep(1 + random.random())\n",
    "        except Exception as nav_e:\n",
    "            print(f\"  Error navigating to biography URL '{champion_data['bio_url']}': {nav_e}\")\n",
    "            return champion_data\n",
    "        \n",
    "        clicked_scroll_button = False\n",
    "        try:\n",
    "            button_selector = (By.CSS_SELECTOR, \"p.cta_VVdh\")\n",
    "            scroll_button = WebDriverWait(self.driver, 7).until(\n",
    "                EC.presence_of_element_located(button_selector)\n",
    "            )\n",
    "            print(\"  'Scroll to Begin' button (p.cta_VVdh) is present.\")\n",
    "            try:\n",
    "                self.driver.execute_script(\"arguments[0].scrollIntoView({block: 'center', inline: 'nearest'});\", scroll_button)\n",
    "                time.sleep(1.0)\n",
    "                self.driver.execute_script(\"arguments[0].click();\", scroll_button)\n",
    "                print(\"  Attempted click 'Scroll to Begin' button via JavaScript.\")\n",
    "                clicked_scroll_button = True\n",
    "                print(\"  Performing small scroll down after click...\")\n",
    "                self.driver.execute_script(\"window.scrollBy(0, 150);\")\n",
    "                time.sleep(0.5)\n",
    "            except Exception as js_click_e:\n",
    "                print(f\"  Warn: JavaScript click execution failed: {type(js_click_e).__name__} - {js_click_e}\")\n",
    "        except TimeoutException:\n",
    "            print(\"  Info: 'Scroll to Begin' button (p.cta_VVdh) not found within timeout.\")\n",
    "        except Exception as scroll_e:\n",
    "            print(f\"  Warn: Error interacting with 'Scroll to Begin' button: {type(scroll_e).__name__} - {scroll_e}\")\n",
    "\n",
    "        primary_paragraph_selector = \"p.p_1_sJ\"\n",
    "        container_selector = \"#CatchElement\"\n",
    "        bio_text, para_count = self.extract_page_content(container_selector, primary_paragraph_selector)\n",
    "        champion_data['full_biography'] = bio_text\n",
    "\n",
    "        if bio_text:\n",
    "            actual_paragraphs = len(bio_text.split('\\n\\n'))\n",
    "            print(f\"  Extracted biography text ({actual_paragraphs} non-empty paragraphs joined).\")\n",
    "        elif not bio_text:\n",
    "            print(f\"  Warn: Failed to extract biography text content from '{container_selector}'.\")\n",
    "\n",
    "        story_button_found = False\n",
    "        try:\n",
    "            story_links = self.driver.find_elements(By.XPATH, \n",
    "                \"//a[.//button[.//span[contains(text(), 'story') or contains(text(), 'Story')]]]|\" +\n",
    "                \"//a[contains(@href,'/story/')][not(contains(@href, '/story/champion/'))]|\" +\n",
    "                \"//a[contains(@href,'-color-story')]\"\n",
    "            )\n",
    "            \n",
    "            found_story_url = \"\"\n",
    "            if story_links:\n",
    "                for link in story_links:\n",
    "                    href = link.get_attribute('href')\n",
    "                    if href and ('/story/' in href) and ('/story/champion/' not in href):\n",
    "                        found_story_url = href\n",
    "                        story_button_found = True\n",
    "                        break\n",
    "                \n",
    "                if not found_story_url and story_links:\n",
    "                    found_story_url = story_links[0].get_attribute('href')\n",
    "                    story_button_found = True\n",
    "            \n",
    "            if found_story_url:\n",
    "                champion_data['story_url'] = found_story_url\n",
    "                print(f\"  Found story URL on bio page: {champion_data['story_url']}\")\n",
    "            else:\n",
    "                print(f\"  No story link found on bio page.\")\n",
    "                champion_data['story_url'] = \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"  Warn: Error finding story link on bio page: {e}\")\n",
    "            champion_data['story_url'] = \"\"\n",
    "            \n",
    "        if not story_button_found and not champion_data['story_url']:\n",
    "            try:\n",
    "                clean_name = re.sub(r'[^a-z0-9]', '', champion_data['name'].lower())\n",
    "                fallback_url = f\"{self.base_url}/en_US/story/{clean_name}-color-story/\"\n",
    "                print(f\"  Creating fallback story URL: {fallback_url}\")\n",
    "                champion_data['story_url'] = fallback_url\n",
    "            except Exception as fallback_e:\n",
    "                print(f\"  Error creating fallback story URL: {fallback_e}\")\n",
    "                champion_data['story_url'] = \"\"\n",
    "                \n",
    "        return champion_data\n",
    "\n",
    "    def extract_story_content(self, champion_data):\n",
    "        \"\"\"Extract the full story content from the story URL\"\"\"\n",
    "        champion_data['full_story'] = \"\"\n",
    "        if not champion_data.get('story_url') or not champion_data['story_url'].startswith(self.base_url):\n",
    "            print(f\"  Info: No valid story URL available for {champion_data['name']}\")\n",
    "            return champion_data\n",
    "\n",
    "        print(f\"Navigating to story page for {champion_data['name']}...\")\n",
    "        try:\n",
    "            self.driver.get(champion_data['story_url'])\n",
    "            WebDriverWait(self.driver, 15).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"body\")))\n",
    "            time.sleep(1 + random.random())\n",
    "        except Exception as nav_e:\n",
    "            print(f\"  Error navigating to story URL '{champion_data['story_url']}': {nav_e}\")\n",
    "            return champion_data\n",
    "\n",
    "        clicked_scroll_button = False\n",
    "        try:\n",
    "            button_selector = (By.CSS_SELECTOR, \"p.cta_VVdh\")\n",
    "            scroll_button = WebDriverWait(self.driver, 7).until(\n",
    "                EC.presence_of_element_located(button_selector)\n",
    "            )\n",
    "            print(\"  'Scroll to Begin' button (p.cta_VVdh) is present.\")\n",
    "            try:\n",
    "                self.driver.execute_script(\"arguments[0].scrollIntoView({block: 'center', inline: 'nearest'});\", scroll_button)\n",
    "                time.sleep(1.0)\n",
    "                self.driver.execute_script(\"arguments[0].click();\", scroll_button)\n",
    "                print(\"  Attempted click 'Scroll to Begin' button via JavaScript.\")\n",
    "                clicked_scroll_button = True\n",
    "                print(\"  Performing small scroll down after click...\")\n",
    "                self.driver.execute_script(\"window.scrollBy(0, 150);\")\n",
    "                time.sleep(0.5)\n",
    "            except Exception as js_click_e:\n",
    "                print(f\"  Warn: JavaScript click execution failed: {type(js_click_e).__name__} - {js_click_e}\")\n",
    "        except TimeoutException:\n",
    "            print(\"  Info: 'Scroll to Begin' button (p.cta_VVdh) not found within timeout.\")\n",
    "        except Exception as scroll_e:\n",
    "            print(f\"  Warn: Error interacting with 'Scroll to Begin' button: {type(scroll_e).__name__} - {scroll_e}\")\n",
    "\n",
    "        primary_paragraph_selector = \"p.p_1_sJ\"\n",
    "        container_selector = \"#CatchElement\"\n",
    "        story_text, para_count = self.extract_page_content(container_selector, primary_paragraph_selector)\n",
    "        champion_data['full_story'] = story_text\n",
    "\n",
    "        if story_text:\n",
    "            actual_paragraphs = len(story_text.split('\\n\\n'))\n",
    "            print(f\"  Extracted story text ({actual_paragraphs} non-empty paragraphs joined).\")\n",
    "        elif not story_text:\n",
    "            print(f\"  Warn: Failed to extract story text content from '{container_selector}'.\")\n",
    "\n",
    "        return champion_data\n",
    "\n",
    "    def scrape_champions(self, limit=None):\n",
    "        \"\"\"Scrape information for all champions\"\"\"\n",
    "        all_data = []\n",
    "        try:\n",
    "            champions_list = self.extract_champions_list()\n",
    "            if not champions_list:\n",
    "                print(\"Error: Failed to extract champions list. Exiting.\")\n",
    "                return []\n",
    "            if limit:\n",
    "                champions_list = champions_list[:limit]\n",
    "            self.champions_data = []\n",
    "\n",
    "            for champion in tqdm(champions_list, desc=\"Scraping champions\"):\n",
    "                time.sleep(1.5 + random.random() * 2)\n",
    "                current_champion_data = {'name': champion['name'], 'url': champion['url'], 'region': champion.get('region','')}\n",
    "                current_champion_data = self.extract_champion_details(current_champion_data)\n",
    "                current_champion_data = self.extract_bio_and_story(current_champion_data)\n",
    "                current_champion_data = self.extract_story_content(current_champion_data)\n",
    "                all_data.append(current_champion_data)\n",
    "                self.champions_data = all_data\n",
    "                self.save_to_json(self.champions_data, 'data/progress_champions_data.json') # Pass data\n",
    "\n",
    "            print(f\"\\nScraping complete. Processed {len(all_data)} champions.\")\n",
    "            return all_data\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nScraping interrupted by user.\")\n",
    "            return all_data\n",
    "        except Exception as e:\n",
    "            print(f\"\\nAn critical error occurred during scraping: {type(e).__name__} - {e}\")\n",
    "            traceback.print_exc()\n",
    "            return all_data\n",
    "        finally:\n",
    "            print(\"Closing WebDriver...\")\n",
    "            if hasattr(self, 'driver'):\n",
    "                self.driver.quit()\n",
    "\n",
    "    def save_to_csv(self, data_to_save, filename='data/lol_champions_data.csv'):\n",
    "        \"\"\"Save the collected data to a CSV file\"\"\"\n",
    "        if not data_to_save:\n",
    "            print(\"No champion data provided to save to CSV.\")\n",
    "            return\n",
    "        try:\n",
    "            df = pd.DataFrame(data_to_save)\n",
    "            cols = ['name', 'region', 'role', 'race', 'quote', 'related_champions',\n",
    "                    'short_bio', 'full_biography', 'full_story', 'url', 'bio_url', 'story_url']\n",
    "            if 'related_champions' in df.columns:\n",
    "                df['related_champions'] = df['related_champions'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "            df = df.reindex(columns=[col for col in cols if col in df.columns])\n",
    "            df.to_csv(filename, index=False, encoding='utf-8')\n",
    "            print(f\"Data saved to {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data to CSV {filename}: {e}\")\n",
    "\n",
    "    def save_to_json(self, data_to_save, filename='data/lol_champions_data.json'):\n",
    "        \"\"\"Save the collected data to a JSON file\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data_to_save, f, ensure_ascii=False, indent=4)\n",
    "            if 'progress' not in filename: print(f\"Data saved to {filename}\")\n",
    "            else: pass\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data to JSON {filename}: {e}\")\n",
    "\n",
    "scraper = LoLChampionScraper()\n",
    "final_champion_data = None\n",
    "try:\n",
    "    final_champion_data = scraper.scrape_champions()\n",
    "finally:\n",
    "    if final_champion_data:\n",
    "        print(\"\\nSaving final data...\")\n",
    "        scraper.save_to_csv(final_champion_data, 'data/lol_champions_data.csv')\n",
    "        scraper.save_to_json(final_champion_data, 'data/lol_champions_data.json')\n",
    "    else:\n",
    "        if scraper.champions_data:\n",
    "            print(\"\\nScraping did not complete fully, saving data collected so far...\")\n",
    "            scraper.save_to_csv(scraper.champions_data, 'data/lol_champions_data.csv')\n",
    "            scraper.save_to_json(scraper.champions_data, 'data/lol_champions_data.json')\n",
    "        else:\n",
    "            print(\"\\nNo final data collected to save.\")\n",
    "    if hasattr(scraper, 'driver') and scraper.driver:\n",
    "        scraper.driver.quit()\n",
    "        print(\"WebDriver quit confirmed from main.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f817ae5",
   "metadata": {},
   "source": [
    "## 2. Getting the Model (GPT-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58bd91a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "276d32f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading GPT-2 XL tokenizer for openai-community/gpt2-xl …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e92f91c4e0b47f7b562f3936fb2b063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5b278921024030b6c8dd5d4665dfaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec6892ef0cf4ed4bfb6f31d44f47e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfbce6d1d3764d3e929052ecfc139727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1daccd3270e4e9783dddeb0db72d1e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPT-2 XL tokenizer saved to models/gpt2-xl/\n",
      "\n",
      "Downloading GPT-2 XL model for openai-community/gpt2-xl …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6274bea4ed0a4d54bc67eb13232a3fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364ea5274c5a4b0691e4911b531f5c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPT-2 XL model saved to models/gpt2-xl/\n",
      "\n",
      "GPT-2 XL download complete.\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "GPT2_ID    = \"openai-community/gpt2-xl\"\n",
    "GPT2_LOCAL = \"models/gpt2-xl\"\n",
    "\n",
    "def reset_dir(path):\n",
    "    if os.path.exists(path):\n",
    "        print(f\"Removing existing directory: {path}/\")\n",
    "        shutil.rmtree(path)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    reset_dir(GPT2_LOCAL)\n",
    "\n",
    "    print(f\"Downloading GPT-2 XL tokenizer for {GPT2_ID} …\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(GPT2_ID)\n",
    "    tokenizer.save_pretrained(GPT2_LOCAL)\n",
    "    print(f\"✅ GPT-2 XL tokenizer saved to {GPT2_LOCAL}/\\n\")\n",
    "\n",
    "    print(f\"Downloading GPT-2 XL model for {GPT2_ID} …\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(GPT2_ID)\n",
    "    model.save_pretrained(GPT2_LOCAL)\n",
    "    print(f\"✅ GPT-2 XL model saved to {GPT2_LOCAL}/\\n\")\n",
    "\n",
    "    print(\"GPT-2 XL download complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ee3b72",
   "metadata": {},
   "source": [
    "## 3. Generation with the Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9601d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import modal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc9514c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3475dc8d8a37447caf9ff6fdb265d14d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the original model\n",
    "original_model_path = \"models/gpt2-xl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(original_model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(original_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "306740fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1600)\n",
       "    (wpe): Embedding(1024, 1600)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-47): 48 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=4800, nx=1600)\n",
       "          (c_proj): Conv1D(nf=1600, nx=1600)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=6400, nx=1600)\n",
       "          (c_proj): Conv1D(nf=1600, nx=6400)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the device and turn on evaluation mode\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e02e0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "\n",
    "# An image based on the official CUDA‑PyTorch container + extras\n",
    "image = (\n",
    "    modal.Image.from_registry(\"nvcr.io/nvidia/pytorch:24.02-py3\")  # CUDA 12 + PyTorch 2.2\n",
    "    .pip_install(\n",
    "        \"transformers==4.40.1\",\n",
    "        \"sentencepiece\",         \n",
    "        \"accelerate\",           \n",
    "    )\n",
    ")\n",
    "app = modal.App(\"gpt2-xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aed2e375",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.function(\n",
    "    image=image,        \n",
    "    gpu=\"H100\",\n",
    "    timeout=300\n",
    ")\n",
    "def generate_text(prompt: str):\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    import torch\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # load from HF hub (fast, streamed) — no need to copy local weights\n",
    "    model_id = \"gpt2-xl\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=200,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.9,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2510390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in the world of League of Legends, the game's community thought it was awesome that one of the game's most popular champions, Jayce, had a very short history, as well as a very long, rich one.\n",
      "\n",
      "The game's lore went from a simple one-line saying to a detailed one-line summary of Jayce's origins and development as a character over the course of a decade and a half, and it's only gotten bigger since that time. While there are a lot of \"what if\" questions about how Jayce's history and development would have played out, this article will explore what might have happened if Jayce was the first champion we played.\n",
      "\n",
      "Jayce's beginnings\n",
      "\n",
      "At the beginning of League of Legends, Jayce was a support mage. He was played by the player known only as TheOddOne, and his origins were outlined in this guide, which you can read for a lot of the background information on\n"
     ]
    }
   ],
   "source": [
    "with app.run():      \n",
    "    story = generate_text.remote(\n",
    "        \"Once upon a time in the world of League of Legends,\"\n",
    "    )\n",
    "    print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2e0faef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in the world of League of Legends, on the land of Runeterra, there lived a champion named Zed. Zed was not an ordinary champion. Zed was a living demon, an unstoppable force of chaos and destruction. He was also a powerful, terrifying human, and he had a knack for getting along with almost everyone he met. Zed and his team were members of the League of Legends, the elite fighting force for which Runeterra had become famous. Zed was a deadly soldier, a master of the assassin's role, and a champion who had earned his place as a legend. And while Zed himself wasn't an ordinary champion, he was an ordinary assassin, and a very ordinary one at that.\n",
      "\n",
      "The first time Zed saw Elise, she was standing by the side of the river, tending to her garden. Zed had come across her in the jungle, and he had quickly come to her aid as she was being attacked by a nearby camp. Elise, naturally\n"
     ]
    }
   ],
   "source": [
    "with app.run():          # or `with app.run():` if in same file\n",
    "    story = generate_text.remote(\n",
    "        \"Once upon a time in the world of League of Legends, on the land of Runeterra, there lived a champion named\"\n",
    "    )\n",
    "    print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdb0d3c",
   "metadata": {},
   "source": [
    "## 4. Fine-tuning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91ceb0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "878ca13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"data\")\n",
    "parquet_path = data_dir / \"lol_champions_data.parquet\"\n",
    "\n",
    "df = pd.read_parquet(parquet_path)\n",
    "df[\"text_to_embed\"] = (\n",
    "    df[\"name\"].fillna(\"\")     + \" — \" +\n",
    "    df[\"role\"].fillna(\"\")     + \"\\n\" +\n",
    "    df[\"race\"].fillna(\"\")     + \"\\n\" +\n",
    "    df[\"short_bio\"].fillna(\"\")+ \"\\n\" +\n",
    "    df[\"full_story\"].fillna(\"\")\n",
    ")\n",
    "texts = df[\"text_to_embed\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7edd3248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c85f10bdaf434d84a22f218e273586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/170 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 170\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"models/gpt2-xl\"\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "ds = Dataset.from_dict({\"text\": texts})\n",
    "\n",
    "# GPT-2's max context length is 1024\n",
    "context_length = 1024\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        padding = \"max_length\"\n",
    "    )\n",
    "\n",
    "ds_tok = ds.map(\n",
    "    tokenize, batched=True, remove_columns=[\"text\"]\n",
    ")\n",
    "ds_tok.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\"]\n",
    ")\n",
    "ds_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ced88a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e6aefb4a144bfe8953bf9bc4b96806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/170 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset saved to data/tokenised_ds\n"
     ]
    }
   ],
   "source": [
    "# Save the tokenized dataset to disk\n",
    "output_dir = Path(\"./data/tokenised_ds\")\n",
    "ds_tok.save_to_disk(output_dir)\n",
    "print(f\"Tokenized dataset saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4043eaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and load dataset from local_disk\n",
    "vol = modal.Volume.from_name(\"data\", create_if_missing=True)\n",
    "\n",
    "with vol.batch_upload() as batch:\n",
    "    batch.put_directory(\"./data/tokenised_ds\", \"/tokenized_ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ede931cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_app = modal.App(\"gpt2-xl-ft\")\n",
    "ft_image = modal.Image.debian_slim().pip_install(\n",
    "    \"transformers\", \"datasets\", \"accelerate\", \n",
    "    \"bitsandbytes\", \"peft\", \"torch\"\n",
    ")\n",
    "GPU = \"H100\"\n",
    "vol = modal.Volume.from_name(\"data\")\n",
    "model_vol = modal.Volume.from_name(\n",
    "    \"gpt2_ft\", create_if_missing=True\n",
    ")\n",
    "\n",
    "@fine_tune_app.function(\n",
    "    image=ft_image,\n",
    "    gpu=GPU,\n",
    "    timeout = 3600,\n",
    "    volumes={\"/data\": vol, \"/checkpoints\": model_vol}\n",
    ")\n",
    "def train_model():\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "    from datasets import Dataset\n",
    "    from datasets import load_from_disk\n",
    "    \n",
    "    # Load tokenized dataset from volume\n",
    "    ds_tok = load_from_disk(\"/data/tokenized_ds\")\n",
    "\n",
    "    ds_tok = ds_tok.map(lambda ex: {\"label\": ex[\"input_ids\"]})\n",
    "    \n",
    "    # Get the model\n",
    "    model_name = \"gpt2-xl\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, pad_token_id = 50256)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer      = tokenizer,\n",
    "        mlm            = False,   # causal, not BERT‑style MLM\n",
    "        return_tensors = \"pt\"\n",
    "    )\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        # foler where training artifacts go\n",
    "        output_dir = \"/checkpoints\",\n",
    "        # micro-batch size on each GPU\n",
    "        per_device_train_batch_size = 1,\n",
    "        # number of micro‑batches to accumulate before calling optimizer.step()\n",
    "        gradient_accumulation_steps = 8,\n",
    "        # number of epochs\n",
    "        num_train_epochs = 3,\n",
    "        # learning rate\n",
    "        learning_rate = 5e-5,\n",
    "        # use mixed precision (for faster training)\n",
    "        bf16 = True, # H100\n",
    "        # log every 50 steps\n",
    "        logging_steps = 50,\n",
    "        # no eval loops\n",
    "        # evaluation_strategy = \"no\",     \n",
    "        save_strategy = \"epoch\"  \n",
    "    ) \n",
    "\n",
    "    print(f\"Training model with dataset: {ds_tok}\")\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=ds_tok,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    trainer.save_model(\"/checkpoints/final\")\n",
    "    tokenizer.save_pretrained(\"/checkpoints/final\")\n",
    "    print(\"Fine-tune complete.  Model saved to /checkpoints/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f3278644",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fine_tune_app.run():\n",
    "    train_model.remote()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feaa7f3",
   "metadata": {},
   "source": [
    "## 5. Fine-tuned Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "81538f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vol = modal.Volume.from_name(\"gpt2_ft\")\n",
    "\n",
    "local_dir = Path(\"./models/gpt2-xl-finetuned-full\")\n",
    "local_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70797459",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download the model to local directory\n",
    "## !modal volume get gpt2_ft /final ./models/gpt2-xl-finetuned-full "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3b0fa30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dp/j76v96ld7kgfdxz859wprxkw0000gn/T/ipykernel_8647/864885668.py:16: DeprecationError: 2025-02-03: Modal will stop implicitly adding local Python modules to the Image (\"automounting\") in a future update. The following modules need to be explicitly added for future compatibility:\n",
      "* _remote_module_non_scriptable\n",
      "\n",
      "e.g.:\n",
      "image_with_source = my_image.add_local_python_source(\"_remote_module_non_scriptable\")\n",
      "\n",
      "For more information, see https://modal.com/docs/guide/modal-1-0-migration\n",
      "  def ft_inference(prompt, max_length=200):\n"
     ]
    }
   ],
   "source": [
    "ft_inference_app = modal.App(\"gpt2-xl-ft-inference\")\n",
    "GPU = \"H100\"\n",
    "ft_inference_image = modal.Image.debian_slim().pip_install(\n",
    "    \"transformers==4.40.1\",\n",
    "    \"sentencepiece\",         \n",
    "    \"accelerate\",  \n",
    ")\n",
    "ft_inference_vol = modal.Volume.from_name(\"gpt2_ft\")\n",
    "\n",
    "@ft_inference_app.function(\n",
    "    image=ft_inference_image,\n",
    "    gpu=GPU,\n",
    "    timeout=3600,\n",
    "    volumes={\"/model\": ft_inference_vol}\n",
    ")\n",
    "def ft_inference(prompt, max_length=200):\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    import torch\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model_path = \"/model/final\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False, padding_side=\"left\", local_files_only=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=200,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.9,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "22d8f9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in the world of League of Legends, the legendary jungler Xerath was the greatest of the Ascended. An immortal god with unfathomable power and an insatiable hunger for the flesh of others, Xerath rose against the Demacian empire with the goal of killing its most revered and beloved figure, the king Jarvan III. After all the deaths he had caused, and all the suffering he had caused, the thought of killing Jarvan had nearly consumed Xerath. But the Ascended had one final test to endure before he could finish off the hated king, and the final challenge was to slay Jarvan himself...\n",
      "It was just after seven, and the streets were still bustling with people. People going about their daily routines, meeting friends and family, and making the most of the last few days of summer before the weather turned.\n",
      "\n",
      "A tall, elegant woman approached a young boy standing in the middle of the crowd. He wore a white dress shirt\n"
     ]
    }
   ],
   "source": [
    "with ft_inference_app.run():      \n",
    "    story = ft_inference.remote(\n",
    "        \"Once upon a time in the world of League of Legends,\"\n",
    "    )\n",
    "    print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4a6cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
