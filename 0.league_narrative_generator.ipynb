{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ab1d55",
   "metadata": {},
   "source": [
    "# League of Legends Narrative Generator\n",
    "### Henry Hu, Suhho Lee, Victor Wei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227ad134",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a36fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping champions: 100%|██████████| 170/170 [19:43<00:00,  6.96s/champion]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import re\n",
    "import traceback\n",
    "\n",
    "class LoLChampionScraper:\n",
    "    def __init__(self):\n",
    "        self.champions_url = \"https://universe.leagueoflegends.com/en_US/champions/\"\n",
    "        self.base_url = \"https://universe.leagueoflegends.com\"\n",
    "        self.champions_data = []\n",
    "\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.88 Safari/537.36\") # Example user agent\n",
    "        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=self.chrome_options)\n",
    "        print(\"WebDriver Initialized\")\n",
    "\n",
    "    def extract_champions_list(self):\n",
    "        \"\"\"Extract list of champions using Selenium\"\"\"\n",
    "        self.driver.get(self.champions_url)\n",
    "        time.sleep(5)\n",
    "        selectors = [\n",
    "            \"li.item_30l8 a\",\n",
    "            \".champsListUl_2Lmb li a\",\n",
    "            \"a[href*='/champion/']\"\n",
    "        ]\n",
    "        champions = []\n",
    "        timeout = 10\n",
    "        for selector in selectors:\n",
    "            try:\n",
    "                WebDriverWait(self.driver, timeout).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
    "                )\n",
    "                champion_elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                if champion_elements:\n",
    "                    for element in champion_elements:\n",
    "                        try:\n",
    "                            url = element.get_attribute(\"href\")\n",
    "                            if not url or not url.startswith(self.base_url):\n",
    "                                continue\n",
    "                            name_element = element.find_element(By.CSS_SELECTOR, \"h1\") if element.find_elements(By.CSS_SELECTOR, \"h1\") else None\n",
    "                            region_element = element.find_element(By.CSS_SELECTOR, \"h2\") if element.find_elements(By.CSS_SELECTOR, \"h2\") else None\n",
    "                            name = name_element.text.strip() if name_element and name_element.text else \"\"\n",
    "                            region = region_element.text.strip() if region_element and region_element.text else \"\"\n",
    "                            if name and url and not any(c['name'] == name.upper() for c in champions):\n",
    "                                champions.append({'name': name.upper(), 'region': region, 'url': url})\n",
    "                        except Exception as e: print(f\"  Warn: Error processing a champion list element: {e}\")\n",
    "                    if champions:\n",
    "                        print(f\"  Successfully extracted champion list using selector: {selector}\")\n",
    "                        break\n",
    "            except TimeoutException: print(f\"  Selector {selector} timed out.\")\n",
    "            except Exception as e: print(f\"  Selector {selector} failed with error: {e}\")\n",
    "        print(f\"Found {len(champions)} unique champions\")\n",
    "        return champions\n",
    "\n",
    "    def extract_champion_details(self, champion_data):\n",
    "        \"\"\"Extract detailed information for a specific champion's main page\"\"\"\n",
    "        print(f\"Extracting details for {champion_data['name']}...\")\n",
    "        if not champion_data.get('url'):\n",
    "            print(f\"  Error: Missing URL for {champion_data['name']}\")\n",
    "            return champion_data\n",
    "        try:\n",
    "            self.driver.get(champion_data['url'])\n",
    "            WebDriverWait(self.driver, 15).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"body\")))\n",
    "            time.sleep(1 + random.random())\n",
    "        except Exception as e:\n",
    "            print(f\"  Error navigating to champion page {champion_data['url']}: {e}\")\n",
    "            return champion_data\n",
    "\n",
    "        # Extract Role, Race, Quote, Short Bio\n",
    "        try:\n",
    "            role_element = WebDriverWait(self.driver, 5).until(EC.presence_of_element_located((By.CSS_SELECTOR, \".typeDescription_ixWu h6, .playerType_3laO h6\")))\n",
    "            champion_data['role'] = role_element.text.strip()\n",
    "        except: champion_data['role'] = \"\"\n",
    "        try:\n",
    "            race_elements = self.driver.find_elements(By.CSS_SELECTOR, \".ChampionRace_a_Fp h6, .race_3k58 h6\")\n",
    "            champion_data['race'] = race_elements[0].text.strip() if race_elements else \"\"\n",
    "        except: champion_data['race'] = \"\"\n",
    "        try:\n",
    "            quote_elements = self.driver.find_elements(By.CSS_SELECTOR, \".quote_2507 p, .championQuotes_3FLE p\")\n",
    "            champion_data['quote'] = quote_elements[0].text.strip() if quote_elements else \"\"\n",
    "        except: champion_data['quote'] = \"\"\n",
    "        \n",
    "        # IMPROVED: Extract short bio with multiple selectors and approaches\n",
    "        try:\n",
    "            # First try the original approach\n",
    "            bio_elements = self.driver.find_elements(By.CSS_SELECTOR, \".biographyText_3-to p, .biography_3YIe p\")\n",
    "            if bio_elements and bio_elements[0].text.strip():\n",
    "                champion_data['short_bio'] = bio_elements[0].text.strip()\n",
    "            else:\n",
    "                # Try getting the text from the div container directly\n",
    "                bio_containers = self.driver.find_elements(By.CSS_SELECTOR, \".biographyText_3-to, .biography_3YIe\")\n",
    "                if bio_containers:\n",
    "                    container_text = bio_containers[0].text.strip()\n",
    "                    if container_text:\n",
    "                        # Split by newlines and take the first paragraph if multiple exist\n",
    "                        paragraphs = [p.strip() for p in container_text.split('\\n') if p.strip()]\n",
    "                        if paragraphs:\n",
    "                            champion_data['short_bio'] = paragraphs[0]\n",
    "                        else:\n",
    "                            champion_data['short_bio'] = container_text\n",
    "                    else:\n",
    "                        champion_data['short_bio'] = \"\"\n",
    "                else:\n",
    "                    champion_data['short_bio'] = \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"  Warn: Error extracting short bio: {e}\")\n",
    "            champion_data['short_bio'] = \"\"\n",
    "\n",
    "        # Extract Related Champions\n",
    "        related_champions = []\n",
    "        try:\n",
    "            h5_locator = (By.CSS_SELECTOR, \"ul.champions_jmhN li.champion_1xlO h5\")\n",
    "            WebDriverWait(self.driver, 3).until(EC.presence_of_element_located(h5_locator))\n",
    "            related_elements = self.driver.find_elements(*h5_locator)\n",
    "            if related_elements:\n",
    "                for i, elem in enumerate(related_elements):\n",
    "                    try:\n",
    "                        champion_name = self.driver.execute_script(\"return arguments[0].textContent;\", elem).strip()\n",
    "                        if champion_name and champion_name not in related_champions:\n",
    "                            related_champions.append(champion_name)\n",
    "                    except Exception as inner_e: \n",
    "                        print(f\"    Warn: Error processing related champion element {i+1}: {type(inner_e).__name__} - {inner_e}\")\n",
    "        except Exception as e: \n",
    "            print(f\"  Warn: An unexpected error occurred while finding/processing related champions: {type(e).__name__} - {e}\")\n",
    "        champion_data['related_champions'] = related_champions\n",
    "        print(f\"  Assigned related champions list: {champion_data['related_champions']}\")\n",
    "\n",
    "        # Find Biography URL\n",
    "        try:\n",
    "            bio_link_elements = self.driver.find_elements(By.XPATH, \"//a[.//button[.//span[contains(text(), 'Read Biography') or contains(text(), 'Read Bio')]]]|//a[contains(@href,'/story/champion/')]\")\n",
    "            found_bio_url = \"\"\n",
    "            if bio_link_elements:\n",
    "                for link_el in bio_link_elements:\n",
    "                    href = link_el.get_attribute('href')\n",
    "                    if href and '/story/champion/' in href: found_bio_url = href; break\n",
    "                if not found_bio_url: found_bio_url = bio_link_elements[0].get_attribute('href')\n",
    "            if found_bio_url: champion_data['bio_url'] = found_bio_url\n",
    "            else:\n",
    "                clean_name = re.sub(r'[^a-z0-9]', '', champion_data['name'].lower()); bio_url = f\"{self.base_url}/en_US/story/champion/{clean_name}/\"; champion_data['bio_url'] = bio_url; print(f\"  Warn: Could not find bio button/link, constructed fallback URL: {bio_url}\")\n",
    "        except Exception as e: \n",
    "            print(f\"  Warn: Could not find or construct biography URL: {e}\"); champion_data['bio_url'] = \"\"\n",
    "        champion_data['story_url'] = \"\"\n",
    "        return champion_data\n",
    "\n",
    "    def extract_page_content(self, container_selector, paragraph_selector):\n",
    "        \"\"\"Helper function to extract joined paragraph text from a container.\"\"\"\n",
    "        full_text = \"\"\n",
    "        paragraphs_count = 0\n",
    "        try:\n",
    "            print(f\"  Attempting to find container '{container_selector}' directly in DOM...\")\n",
    "            container_elements = self.driver.find_elements(By.CSS_SELECTOR, container_selector)\n",
    "\n",
    "            if not container_elements:\n",
    "                print(f\"  Error: Container '{container_selector}' not found in DOM after interaction attempt.\")\n",
    "                return full_text, paragraphs_count\n",
    "\n",
    "            container_element = container_elements[0]\n",
    "            print(f\"  Container '{container_selector}' found in DOM.\")\n",
    "\n",
    "            paragraphs = container_element.find_elements(By.CSS_SELECTOR, paragraph_selector)\n",
    "            paragraphs_count = len(paragraphs)\n",
    "            if paragraphs:\n",
    "                extracted_texts = []\n",
    "                for i, p in enumerate(paragraphs):\n",
    "                    try:\n",
    "                        para_text = self.driver.execute_script(\n",
    "                            \"return arguments[0].textContent;\", p\n",
    "                        ).strip()\n",
    "                        if para_text:\n",
    "                            extracted_texts.append(para_text)\n",
    "                    except Exception as inner_e:\n",
    "                        print(f\"    Warn: Error processing paragraph {i+1}: {type(inner_e).__name__} - {inner_e}\")\n",
    "\n",
    "                full_text = \"\\n\\n\".join(extracted_texts)\n",
    "                if not full_text and paragraphs_count > 0:\n",
    "                    print(f\"  Warn: Found {paragraphs_count} paragraphs in '{container_selector}', but all textContent was empty after processing.\")\n",
    "            else:\n",
    "                print(f\"  Warn: Container '{container_selector}' found, but no paragraphs matched selector '{paragraph_selector}'.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error: Exception finding/processing content within '{container_selector}': {type(e).__name__} - {e}\")\n",
    "\n",
    "        return full_text, paragraphs_count\n",
    "\n",
    "    def extract_bio_and_story(self, champion_data):\n",
    "        \"\"\"Extract full biography from bio_url and find the story_url.\"\"\"\n",
    "        champion_data['full_biography'] = \"\"\n",
    "        if not champion_data.get('bio_url'):\n",
    "            print(f\"  Info: No biography URL available for {champion_data['name']}\")\n",
    "            return champion_data\n",
    "\n",
    "        print(f\"Navigating to biography page for {champion_data['name']}...\")\n",
    "        try:\n",
    "            self.driver.get(champion_data['bio_url'])\n",
    "            WebDriverWait(self.driver, 15).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"body\")))\n",
    "            time.sleep(1 + random.random())\n",
    "        except Exception as nav_e:\n",
    "            print(f\"  Error navigating to biography URL '{champion_data['bio_url']}': {nav_e}\")\n",
    "            return champion_data\n",
    "        \n",
    "        clicked_scroll_button = False\n",
    "        try:\n",
    "            button_selector = (By.CSS_SELECTOR, \"p.cta_VVdh\")\n",
    "            scroll_button = WebDriverWait(self.driver, 7).until(\n",
    "                EC.presence_of_element_located(button_selector)\n",
    "            )\n",
    "            print(\"  'Scroll to Begin' button (p.cta_VVdh) is present.\")\n",
    "            try:\n",
    "                self.driver.execute_script(\"arguments[0].scrollIntoView({block: 'center', inline: 'nearest'});\", scroll_button)\n",
    "                time.sleep(1.0)\n",
    "                self.driver.execute_script(\"arguments[0].click();\", scroll_button)\n",
    "                print(\"  Attempted click 'Scroll to Begin' button via JavaScript.\")\n",
    "                clicked_scroll_button = True\n",
    "                print(\"  Performing small scroll down after click...\")\n",
    "                self.driver.execute_script(\"window.scrollBy(0, 150);\")\n",
    "                time.sleep(0.5)\n",
    "            except Exception as js_click_e:\n",
    "                print(f\"  Warn: JavaScript click execution failed: {type(js_click_e).__name__} - {js_click_e}\")\n",
    "        except TimeoutException:\n",
    "            print(\"  Info: 'Scroll to Begin' button (p.cta_VVdh) not found within timeout.\")\n",
    "        except Exception as scroll_e:\n",
    "            print(f\"  Warn: Error interacting with 'Scroll to Begin' button: {type(scroll_e).__name__} - {scroll_e}\")\n",
    "\n",
    "        primary_paragraph_selector = \"p.p_1_sJ\"\n",
    "        container_selector = \"#CatchElement\"\n",
    "        bio_text, para_count = self.extract_page_content(container_selector, primary_paragraph_selector)\n",
    "        champion_data['full_biography'] = bio_text\n",
    "\n",
    "        if bio_text:\n",
    "            actual_paragraphs = len(bio_text.split('\\n\\n'))\n",
    "            print(f\"  Extracted biography text ({actual_paragraphs} non-empty paragraphs joined).\")\n",
    "        elif not bio_text:\n",
    "            print(f\"  Warn: Failed to extract biography text content from '{container_selector}'.\")\n",
    "\n",
    "        story_button_found = False\n",
    "        try:\n",
    "            story_links = self.driver.find_elements(By.XPATH, \n",
    "                \"//a[.//button[.//span[contains(text(), 'story') or contains(text(), 'Story')]]]|\" +\n",
    "                \"//a[contains(@href,'/story/')][not(contains(@href, '/story/champion/'))]|\" +\n",
    "                \"//a[contains(@href,'-color-story')]\"\n",
    "            )\n",
    "            \n",
    "            found_story_url = \"\"\n",
    "            if story_links:\n",
    "                for link in story_links:\n",
    "                    href = link.get_attribute('href')\n",
    "                    if href and ('/story/' in href) and ('/story/champion/' not in href):\n",
    "                        found_story_url = href\n",
    "                        story_button_found = True\n",
    "                        break\n",
    "                \n",
    "                if not found_story_url and story_links:\n",
    "                    found_story_url = story_links[0].get_attribute('href')\n",
    "                    story_button_found = True\n",
    "            \n",
    "            if found_story_url:\n",
    "                champion_data['story_url'] = found_story_url\n",
    "                print(f\"  Found story URL on bio page: {champion_data['story_url']}\")\n",
    "            else:\n",
    "                print(f\"  No story link found on bio page.\")\n",
    "                champion_data['story_url'] = \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"  Warn: Error finding story link on bio page: {e}\")\n",
    "            champion_data['story_url'] = \"\"\n",
    "            \n",
    "        if not story_button_found and not champion_data['story_url']:\n",
    "            try:\n",
    "                clean_name = re.sub(r'[^a-z0-9]', '', champion_data['name'].lower())\n",
    "                fallback_url = f\"{self.base_url}/en_US/story/{clean_name}-color-story/\"\n",
    "                print(f\"  Creating fallback story URL: {fallback_url}\")\n",
    "                champion_data['story_url'] = fallback_url\n",
    "            except Exception as fallback_e:\n",
    "                print(f\"  Error creating fallback story URL: {fallback_e}\")\n",
    "                champion_data['story_url'] = \"\"\n",
    "                \n",
    "        return champion_data\n",
    "\n",
    "    def extract_story_content(self, champion_data):\n",
    "        \"\"\"Extract the full story content from the story URL\"\"\"\n",
    "        champion_data['full_story'] = \"\"\n",
    "        if not champion_data.get('story_url') or not champion_data['story_url'].startswith(self.base_url):\n",
    "            print(f\"  Info: No valid story URL available for {champion_data['name']}\")\n",
    "            return champion_data\n",
    "\n",
    "        print(f\"Navigating to story page for {champion_data['name']}...\")\n",
    "        try:\n",
    "            self.driver.get(champion_data['story_url'])\n",
    "            WebDriverWait(self.driver, 15).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"body\")))\n",
    "            time.sleep(1 + random.random())\n",
    "        except Exception as nav_e:\n",
    "            print(f\"  Error navigating to story URL '{champion_data['story_url']}': {nav_e}\")\n",
    "            return champion_data\n",
    "\n",
    "        clicked_scroll_button = False\n",
    "        try:\n",
    "            button_selector = (By.CSS_SELECTOR, \"p.cta_VVdh\")\n",
    "            scroll_button = WebDriverWait(self.driver, 7).until(\n",
    "                EC.presence_of_element_located(button_selector)\n",
    "            )\n",
    "            print(\"  'Scroll to Begin' button (p.cta_VVdh) is present.\")\n",
    "            try:\n",
    "                self.driver.execute_script(\"arguments[0].scrollIntoView({block: 'center', inline: 'nearest'});\", scroll_button)\n",
    "                time.sleep(1.0)\n",
    "                self.driver.execute_script(\"arguments[0].click();\", scroll_button)\n",
    "                print(\"  Attempted click 'Scroll to Begin' button via JavaScript.\")\n",
    "                clicked_scroll_button = True\n",
    "                print(\"  Performing small scroll down after click...\")\n",
    "                self.driver.execute_script(\"window.scrollBy(0, 150);\")\n",
    "                time.sleep(0.5)\n",
    "            except Exception as js_click_e:\n",
    "                print(f\"  Warn: JavaScript click execution failed: {type(js_click_e).__name__} - {js_click_e}\")\n",
    "        except TimeoutException:\n",
    "            print(\"  Info: 'Scroll to Begin' button (p.cta_VVdh) not found within timeout.\")\n",
    "        except Exception as scroll_e:\n",
    "            print(f\"  Warn: Error interacting with 'Scroll to Begin' button: {type(scroll_e).__name__} - {scroll_e}\")\n",
    "\n",
    "        primary_paragraph_selector = \"p.p_1_sJ\"\n",
    "        container_selector = \"#CatchElement\"\n",
    "        story_text, para_count = self.extract_page_content(container_selector, primary_paragraph_selector)\n",
    "        champion_data['full_story'] = story_text\n",
    "\n",
    "        if story_text:\n",
    "            actual_paragraphs = len(story_text.split('\\n\\n'))\n",
    "            print(f\"  Extracted story text ({actual_paragraphs} non-empty paragraphs joined).\")\n",
    "        elif not story_text:\n",
    "            print(f\"  Warn: Failed to extract story text content from '{container_selector}'.\")\n",
    "\n",
    "        return champion_data\n",
    "\n",
    "    def scrape_champions(self, limit=None):\n",
    "        \"\"\"Scrape information for all champions\"\"\"\n",
    "        all_data = []\n",
    "        try:\n",
    "            champions_list = self.extract_champions_list()\n",
    "            if not champions_list:\n",
    "                print(\"Error: Failed to extract champions list. Exiting.\")\n",
    "                return []\n",
    "            if limit:\n",
    "                champions_list = champions_list[:limit]\n",
    "            self.champions_data = []\n",
    "\n",
    "            for champion in tqdm(champions_list, desc=\"Scraping champions\"):\n",
    "                time.sleep(1.5 + random.random() * 2)\n",
    "                current_champion_data = {'name': champion['name'], 'url': champion['url'], 'region': champion.get('region','')}\n",
    "                current_champion_data = self.extract_champion_details(current_champion_data)\n",
    "                current_champion_data = self.extract_bio_and_story(current_champion_data)\n",
    "                current_champion_data = self.extract_story_content(current_champion_data)\n",
    "                all_data.append(current_champion_data)\n",
    "                self.champions_data = all_data\n",
    "                self.save_to_json(self.champions_data, 'data/progress_champions_data.json') # Pass data\n",
    "\n",
    "            print(f\"\\nScraping complete. Processed {len(all_data)} champions.\")\n",
    "            return all_data\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nScraping interrupted by user.\")\n",
    "            return all_data\n",
    "        except Exception as e:\n",
    "            print(f\"\\nAn critical error occurred during scraping: {type(e).__name__} - {e}\")\n",
    "            traceback.print_exc()\n",
    "            return all_data\n",
    "        finally:\n",
    "            print(\"Closing WebDriver...\")\n",
    "            if hasattr(self, 'driver'):\n",
    "                self.driver.quit()\n",
    "\n",
    "    def save_to_csv(self, data_to_save, filename='data/lol_champions_data.csv'):\n",
    "        \"\"\"Save the collected data to a CSV file\"\"\"\n",
    "        if not data_to_save:\n",
    "            print(\"No champion data provided to save to CSV.\")\n",
    "            return\n",
    "        try:\n",
    "            df = pd.DataFrame(data_to_save)\n",
    "            cols = ['name', 'region', 'role', 'race', 'quote', 'related_champions',\n",
    "                    'short_bio', 'full_biography', 'full_story', 'url', 'bio_url', 'story_url']\n",
    "            if 'related_champions' in df.columns:\n",
    "                df['related_champions'] = df['related_champions'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "            df = df.reindex(columns=[col for col in cols if col in df.columns])\n",
    "            df.to_csv(filename, index=False, encoding='utf-8')\n",
    "            print(f\"Data saved to {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data to CSV {filename}: {e}\")\n",
    "\n",
    "    def save_to_json(self, data_to_save, filename='data/lol_champions_data.json'):\n",
    "        \"\"\"Save the collected data to a JSON file\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data_to_save, f, ensure_ascii=False, indent=4)\n",
    "            if 'progress' not in filename: print(f\"Data saved to {filename}\")\n",
    "            else: pass\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data to JSON {filename}: {e}\")\n",
    "\n",
    "scraper = LoLChampionScraper()\n",
    "final_champion_data = None\n",
    "try:\n",
    "    final_champion_data = scraper.scrape_champions()\n",
    "finally:\n",
    "    if final_champion_data:\n",
    "        print(\"\\nSaving final data...\")\n",
    "        scraper.save_to_csv(final_champion_data, 'data/lol_champions_data.csv')\n",
    "        scraper.save_to_json(final_champion_data, 'data/lol_champions_data.json')\n",
    "    else:\n",
    "        if scraper.champions_data:\n",
    "            print(\"\\nScraping did not complete fully, saving data collected so far...\")\n",
    "            scraper.save_to_csv(scraper.champions_data, 'data/lol_champions_data.csv')\n",
    "            scraper.save_to_json(scraper.champions_data, 'data/lol_champions_data.json')\n",
    "        else:\n",
    "            print(\"\\nNo final data collected to save.\")\n",
    "    if hasattr(scraper, 'driver') and scraper.driver:\n",
    "        scraper.driver.quit()\n",
    "        print(\"WebDriver quit confirmed from main.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276d32f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
