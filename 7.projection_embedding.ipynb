{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compact Projection of Combined Embeddings\n",
    "We’ll load `combined_raw_embeddings.npy` → train an autoencoder (encoder+decoder) and finally save just the **projected** embeddings from the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dir            = Path(\"data\")\n",
    "combined_path       = data_dir / \"combined_raw_embeddings.npy\"\n",
    "ids_path            = data_dir / \"combined_ids.npy\"\n",
    "projected_out_path  = data_dir / \"combined_projected_embeddings.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded raw: (170, 2136)\n"
     ]
    }
   ],
   "source": [
    "# Load data into a DataLoader\n",
    "raw = np.load(combined_path)              # (N, D_in)\n",
    "ids = np.load(ids_path)                   # (N,)\n",
    "\n",
    "# Convert to torch Tensor and make dataset\n",
    "x_all = torch.from_numpy(raw).float()\n",
    "dataset = TensorDataset(x_all)\n",
    "loader  = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=False)\n",
    "\n",
    "print(\"Loaded raw:\", raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder: 2136 → 512 → 768 → 512 → 2136\n"
     ]
    }
   ],
   "source": [
    "# Define Autoencoder / Projection MLP\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "D_in, H, D_out = raw.shape[1], 512, 768\n",
    "\n",
    "encoder = nn.Sequential(\n",
    "    nn.Linear(D_in, H),\n",
    "    nn.GELU(),\n",
    "    nn.Linear(H, D_out)\n",
    ").to(device)\n",
    "\n",
    "decoder = nn.Sequential(\n",
    "    nn.Linear(D_out, H),\n",
    "    nn.GELU(),\n",
    "    nn.Linear(H, D_in)\n",
    ").to(device)\n",
    "\n",
    "print(f\"Autoencoder: {D_in} → {H} → {D_out} → {H} → {D_in}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 — Avg Reconstruction Loss: 0.105617\n",
      "Epoch 2/5 — Avg Reconstruction Loss: 0.049041\n",
      "Epoch 3/5 — Avg Reconstruction Loss: 0.033984\n",
      "Epoch 4/5 — Avg Reconstruction Loss: 0.029566\n",
      "Epoch 5/5 — Avg Reconstruction Loss: 0.026583\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-3)\n",
    "loss_fn   = nn.MSELoss()\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    total_loss = 0.0\n",
    "    for (xb,) in loader:\n",
    "        xb = xb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        z  = encoder(xb)\n",
    "        xr = decoder(z)\n",
    "        loss = loss_fn(xr, xb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    avg = total_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch}/5 — Avg Reconstruction Loss: {avg:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved decoder weights to data/decoder.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(decoder.state_dict(), \"data/decoder.pth\")\n",
    "print(\"Saved decoder weights to data/decoder.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved projected embeddings: (170, 768)\n"
     ]
    }
   ],
   "source": [
    "# Generate & Save Projected Embeddings\n",
    "with torch.no_grad():\n",
    "    all_raw = torch.from_numpy(raw).float().to(device)\n",
    "    all_proj = encoder(all_raw).cpu().numpy()   # (N, D_out)\n",
    "\n",
    "np.save(projected_out_path, all_proj)\n",
    "print(\"Saved projected embeddings:\", all_proj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projected embeddings shape: (170, 768)\n",
      "Example champion IDs: [0 1 2 3 4]\n",
      "0: [-0.3596 -0.4467  0.5269  0.0525  0.7603  0.0266  0.6629 -0.076  -0.2315\n",
      " -0.2516]\n",
      "1: [-0.3005 -0.2805  0.4057  0.0298  0.6539 -0.0758  0.5893 -0.0717 -0.1746\n",
      " -0.1367]\n",
      "2: [-0.41   -0.4752  0.4427  0.0734  0.6565  0.2281  0.7276 -0.2246 -0.1942\n",
      " -0.0852]\n",
      "3: [-0.3945 -0.4543  0.3965  0.0719  0.6304  0.2515  0.6805 -0.2052 -0.1503\n",
      " -0.0963]\n",
      "4: [-0.1061 -0.2279  0.1393  0.1387  0.6505 -0.2582  0.5135  0.0387  0.0104\n",
      " -0.1911]\n"
     ]
    }
   ],
   "source": [
    "data_dir            = Path(\"data\")\n",
    "proj_path           = data_dir / \"combined_projected_embeddings.npy\"\n",
    "ids_path            = data_dir / \"combined_ids.npy\"\n",
    "\n",
    "# 1. Load\n",
    "proj_embs = np.load(proj_path)   # (N, 768)\n",
    "ids       = np.load(ids_path)    # (N,)\n",
    "\n",
    "# 2. Quick sanity checks\n",
    "print(\"Projected embeddings shape:\", proj_embs.shape)\n",
    "print(\"Example champion IDs:\", ids[:5])\n",
    "\n",
    "# 3. Peek at the first 5 vectors (dims 0–9)\n",
    "for idx in range(5):\n",
    "    champ = ids[idx]\n",
    "    vec   = proj_embs[idx, :10]\n",
    "    print(f\"{champ}: {np.round(vec, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
