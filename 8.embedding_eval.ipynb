{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction MSE: 0.024821\n",
      "Input variance σ² : 0.140893\n",
      "Explained variance: 82.38%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"data\")\n",
    "\n",
    "# 1) Load raw + projected embeddings\n",
    "raw  = np.load(data_dir / \"combined_raw_embeddings.npy\")       # (N, D_in)\n",
    "proj = np.load(data_dir / \"combined_projected_embeddings.npy\") # (N, D_out)\n",
    "\n",
    "# 2) Rebuild the decoder (must match training)\n",
    "D_in, H, D_out = raw.shape[1], 512, proj.shape[1]\n",
    "decoder = nn.Sequential(\n",
    "    nn.Linear(D_out, H),\n",
    "    nn.GELU(),\n",
    "    nn.Linear(H, D_in)\n",
    ")\n",
    "\n",
    "# 3) Load trained weights\n",
    "state = torch.load(data_dir / \"decoder.pth\", map_location=\"cpu\")\n",
    "decoder.load_state_dict(state)\n",
    "decoder.eval()\n",
    "\n",
    "# 4) Reconstruct via the full decoder\n",
    "with torch.no_grad():\n",
    "    proj_tensor = torch.from_numpy(proj).float()         # (N, D_out)\n",
    "    recon_tensor = decoder(proj_tensor)                  # (N, D_in)\n",
    "    recon = recon_tensor.numpy()\n",
    "\n",
    "# 5) Compute MSE & explained variance\n",
    "mse = np.mean((raw - recon)**2)\n",
    "sigma2 = np.mean(raw**2)\n",
    "r2 = 1 - mse / sigma2\n",
    "\n",
    "print(f\"Reconstruction MSE: {mse:.6f}\")\n",
    "print(f\"Input variance σ² : {sigma2:.6f}\")\n",
    "print(f\"Explained variance: {r2:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can try to imporve by using \n",
    "1.\tLonger training: run for more epochs or lower the learning rate.\n",
    "2.\tLarger hidden layer: increase H from 512 → 768 or 1024.\n",
    "3.\tDifferent architecture: add another nonlinearity or use a deeper autoencoder.\n",
    "4.\tContrastive / supervised tuning: if you have downstream labels (“these champions should match”), fine-tune the encoder directly on that objective rather than pure MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
