{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gwei4/miniconda3/envs/kaggle_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Paths\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from node2vec import Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir     = Path(\"data\")\n",
    "parquet_path = data_dir / \"lol_champions_data.parquet\"\n",
    "out_graph    = data_dir / \"graph_embeddings.npy\"\n",
    "out_meta     = data_dir / \"meta_embeddings.npy\"\n",
    "out_names    = data_dir / \"champion_names.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(parquet_path)\n",
    "names = df[\"name\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "G.add_nodes_from(names)\n",
    "name_set = set(names)\n",
    "\n",
    "for row in df.itertuples():\n",
    "    src = row.name\n",
    "    for nbr in (row.related_champions or []):\n",
    "        if nbr in name_set:\n",
    "            G.add_edge(src, nbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 170/170 [00:00<00:00, 87231.67it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 25/25 [00:00<00:00, 3673.67it/s]\n",
      "Generating walks (CPU: 3): 100%|██████████| 25/25 [00:00<00:00, 3621.40it/s]\n",
      "Generating walks (CPU: 2): 100%|██████████| 25/25 [00:00<00:00, 3670.46it/s]\n",
      "Generating walks (CPU: 4): 100%|██████████| 25/25 [00:00<00:00, 3549.08it/s]\n"
     ]
    }
   ],
   "source": [
    "node2vec = Node2Vec(\n",
    "    G,\n",
    "    dimensions=128,\n",
    "    walk_length=30,\n",
    "    num_walks=100,\n",
    "    workers=4,\n",
    "    seed=42\n",
    ")\n",
    "n2v_model = node2vec.fit(window=10, min_count=1, batch_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_embs = np.vstack([n2v_model.wv[name] for name in names])  # (N, 128)\n",
    "\n",
    "meta_df    = pd.get_dummies(df[[\"region\", \"role\", \"race\"]].fillna(\"Unknown\"))\n",
    "meta_embs  = meta_df.values   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(out_graph, graph_embs)\n",
    "np.save(out_meta, meta_embs)\n",
    "np.save(out_names, np.array(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph embeddings shape   : (170, 128)\n",
      "Metadata embeddings shape: (170, 24)\n",
      "Champion names saved to   : data/champion_names.npy\n"
     ]
    }
   ],
   "source": [
    "print(\"Graph embeddings shape   :\", graph_embs.shape)\n",
    "print(\"Metadata embeddings shape:\", meta_embs.shape)\n",
    "print(\"Champion names saved to   :\", out_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For region, role, race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique counts for every column:\n",
      "name                 170\n",
      "region                14\n",
      "role                   6\n",
      "race                   2\n",
      "quote                168\n",
      "related_champions    164\n",
      "short_bio            168\n",
      "full_biography       167\n",
      "full_story           142\n",
      "url                  170\n",
      "bio_url              170\n",
      "story_url            170\n",
      "dtype: int64\n",
      "region               → 14\n",
      "role                 → 6\n",
      "race                 → 2\n",
      "related_champions    → 164\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load your DataFrame\n",
    "data_dir = Path(\"data\")\n",
    "df = pd.read_parquet(data_dir / \"lol_champions_data.parquet\")\n",
    "\n",
    "# Option A: all columns\n",
    "print(\"Unique counts for every column:\")\n",
    "print(df.nunique())\n",
    "\n",
    "# Option B: just the categoricals you care about\n",
    "for col in [\"region\", \"role\", \"race\", \"related_champions\"]:\n",
    "    # if it's a list-column, count unique list-values by converting to tuple\n",
    "    if df[col].dtype == object and isinstance(df[col].dropna().iloc[0], list):\n",
    "        uniq = df[col].dropna().apply(tuple).nunique()\n",
    "    else:\n",
    "        uniq = df[col].nunique(dropna=True)\n",
    "    print(f\"{col:20s} → {uniq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose embedding sizes\n",
    "region_dim, role_dim, race_dim = 8, 5, 3\n",
    "\n",
    "# Build vocab lists\n",
    "region_list = sorted(df[\"region\"].fillna(\"Unknown\").unique())\n",
    "role_list   = sorted(df[\"role\"].fillna(\"Unknown\").unique())\n",
    "race_list   = sorted(df[\"race\"].fillna(\"Unknown\").unique())\n",
    "\n",
    "# Create mapping dicts\n",
    "region2idx = {v:i for i,v in enumerate(region_list)}\n",
    "role2idx   = {v:i for i,v in enumerate(role_list)}\n",
    "race2idx   = {v:i for i,v in enumerate(race_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We’ll keep these on CPU\n",
    "region_embed = nn.Embedding(len(region_list), region_dim)\n",
    "role_embed   = nn.Embedding(len(role_list),   role_dim)\n",
    "race_embed   = nn.Embedding(len(race_list),   race_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame values to index tensors\n",
    "# Map each row’s category to an index\n",
    "region_idx = torch.tensor([region2idx[x] for x in df[\"region\"].fillna(\"Unknown\")], dtype=torch.long)\n",
    "role_idx   = torch.tensor([role2idx[x]   for x in df[\"role\"].fillna(\"Unknown\")],   dtype=torch.long)\n",
    "race_idx   = torch.tensor([race2idx[x]   for x in df[\"race\"].fillna(\"Unknown\")],   dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned meta embeddings shape: (170, 16)\n"
     ]
    }
   ],
   "source": [
    "# Lookup embeddings & build final meta matrix\n",
    "region_embs = region_embed(region_idx)  # (N, region_dim)\n",
    "role_embs   = role_embed(role_idx)      # (N, role_dim)\n",
    "race_embs   = race_embed(race_idx)      # (N, race_dim)\n",
    "\n",
    "# Concatenate and detach\n",
    "meta_learned = torch.cat([region_embs, role_embs, race_embs], dim=1)  # (N, total_dim)\n",
    "meta_learned = meta_learned.detach().cpu().numpy()                    # now safe to numpy()\n",
    "\n",
    "print(\"Learned meta embeddings shape:\", meta_learned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved learned meta embeddings shape: (170, 16)\n"
     ]
    }
   ],
   "source": [
    "# Save learned metadata embeddings\n",
    "np.save(data_dir / \"meta_learned_embeddings.npy\", meta_learned)\n",
    "print(\"Saved learned meta embeddings shape:\", meta_learned.shape)  # (N, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
